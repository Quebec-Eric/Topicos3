{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Aluno:Eric Azevedo de Oliveira\n"
      ],
      "metadata": {
        "id": "4idr7NaVRoXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#imports para normalizacao\n",
        "import unicodedata\n",
        "import re\n",
        "\n",
        "#imports para tokenizacao\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import spacy\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.tokenize import MWETokenizer\n",
        "from textblob import TextBlob\n",
        "from gensim.utils import tokenize\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "from nltk.tokenize.api import TokenizerI\n",
        "from nltk.tokenize import MWETokenizer\n",
        "# Stio words Removal\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n",
        "# stemmer\n",
        "\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "# 6 parte\n",
        "from collections import Counter\n",
        "import csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yX6UGn4WXDO4",
        "outputId": "010558d8-b25b-4611-8752-aa3ca99b243b"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lower case"
      ],
      "metadata": {
        "id": "8LIbwTMRQBYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fazerLowerCasa(linhaArquivo):\n",
        "    return linhaArquivo.lower()"
      ],
      "metadata": {
        "id": "N7xICNOKQDMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accent and diacritic removal"
      ],
      "metadata": {
        "id": "7EDGKHtUQPgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accentAndDiacriticRemoval(linhaArquivo):\n",
        "    nfkd_from=unicodedata.normalize('NFKD',linhaArquivo)\n",
        "    return u\"\".join([c for c in nfkd_from if not unicodedata.combining(c)])\n"
      ],
      "metadata": {
        "id": "KRqXVaCWVJ8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Canonicalizing of acronyms, currency, date and hyphenated words"
      ],
      "metadata": {
        "id": "92jBv023YZzr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def canonicalizing(linhaArquivo):\n",
        "    linhaArquivo = re.sub('\\.(?!(\\S[^. ])|\\d)','', linhaArquivo)\n",
        "    linhaArquivo = re.sub('(?<!\\d)[.,;!?\\'\\(\\)#:-](?!\\d)','',linhaArquivo)\n",
        "    return linhaArquivo\n",
        "\n"
      ],
      "metadata": {
        "id": "PwMD89shYaoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalização"
      ],
      "metadata": {
        "id": "fJULOW-HeuKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Leitura do arquivo\n",
        "\n",
        "with open('Shakespeare.txt', 'r') as arquivoLeitura, open(\"Shakespeare_Normalized.txt\", \"w\") as arquivoEscrita:\n",
        "  for linha in arquivoLeitura:\n",
        "      linha=fazerLowerCasa(linha)\n",
        "      linha=accentAndDiacriticRemoval(linha)\n",
        "      linha=canonicalizing(linha)\n",
        "      arquivoEscrita.write(linha)\n",
        "      arquivoEscrita.flush()\n",
        "\n",
        "arquivoLeitura.close()\n",
        "arquivoEscrita.close()\n"
      ],
      "metadata": {
        "id": "1IlNXQInevU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Tokenização inicio"
      ],
      "metadata": {
        "id": "w2BwASabj8bO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "White Space Tokenization"
      ],
      "metadata": {
        "id": "SFCLxC1bj-od"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def whiteSpace(linhaArquivo):\n",
        "    return linhaArquivo.split()"
      ],
      "metadata": {
        "id": "1jWJ52PHkAjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK: Word Tokenizer"
      ],
      "metadata": {
        "id": "gl9hfbbHlqEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def wordTokenizer(linhaArquivo):\n",
        "    return word_tokenize(linhaArquivo)"
      ],
      "metadata": {
        "id": "SbwC7mYhlq2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK: Tree Bank Tokenizer"
      ],
      "metadata": {
        "id": "wPQdkZDolsAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def treeBankokenizer(linhaArquivo):\n",
        "    token=TreebankWordTokenizer()\n",
        "    return token.tokenize(linhaArquivo)"
      ],
      "metadata": {
        "id": "fwLkAJxlltqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK: Word Punctuation Tokenizer"
      ],
      "metadata": {
        "id": "6jD2Lz4Blt3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def wordPunctuationTokenizer(linhaArquivo):\n",
        "    toke=WordPunctTokenizer()\n",
        "    return toke.tokenize(linhaArquivo)"
      ],
      "metadata": {
        "id": "Dyg9NeZlluiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK: Tweet Tokenizer"
      ],
      "metadata": {
        "id": "-KgmbgfXlvZh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tweetTokenizer(linhaArquivo):\n",
        "    toke= TweetTokenizer()\n",
        "    return toke.tokenize(linhaArquivo)"
      ],
      "metadata": {
        "id": "CxnWqFOelzwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK: MWE Tokenizer"
      ],
      "metadata": {
        "id": "V00m2mfCl1U4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import MWETokenizer\n",
        "\n",
        "def mweTokenizer(linhaArquivo):\n",
        "    tokenizer = MWETokenizer()\n",
        "    return tokenizer.tokenize(linhaArquivo.split())\n"
      ],
      "metadata": {
        "id": "STW3YXbCl1qo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TextBlob Word Tokenizer"
      ],
      "metadata": {
        "id": "0szjnlN3l3ss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def textBlobWordTokenize(linhaArquivo):\n",
        "    text = TextBlob(linhaArquivo)\n",
        "    return text.words\n"
      ],
      "metadata": {
        "id": "dEJbY1Xkl4Mf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "spaCy Tokenizer"
      ],
      "metadata": {
        "id": "Fv2FMLtyl5Em"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def spacyTokenizer(linhaArquivo):\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    dc=nlp(linhaArquivo)\n",
        "    resposta=[token.text for token in dc]\n",
        "    return resposta"
      ],
      "metadata": {
        "id": "0uC2Q1t9l5kO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gensim Word Tokenizer"
      ],
      "metadata": {
        "id": "EZSM-SO5l64Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gensimWordTokenizer(linhaArquivo):\n",
        "    return list(tokenize(linhaArquivo))"
      ],
      "metadata": {
        "id": "YYBFc0cPl7QR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keras Tokenization"
      ],
      "metadata": {
        "id": "OqzExtzQl7vg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def kerasTokenization(linhaArquivo):\n",
        "    return text_to_word_sequence(linhaArquivo)"
      ],
      "metadata": {
        "id": "-P7f1amNl9fs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#enum das funcoes\n",
        "tokes= [\n",
        "    whiteSpace,\n",
        "    wordTokenizer,\n",
        "    treeBankokenizer,\n",
        "    wordPunctuationTokenizer,\n",
        "    tweetTokenizer,\n",
        "    mweTokenizer,\n",
        "    textBlobWordTokenize,\n",
        "    spacyTokenizer,\n",
        "    gensimWordTokenizer,\n",
        "    kerasTokenization\n",
        "]"
      ],
      "metadata": {
        "id": "CHHPmAeRvsYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('Shakespeare_Normalized.txt', 'r') as infile:\n",
        "    text = infile.read()\n",
        "\n",
        "for i, tokenizer in enumerate(tokes):\n",
        "    tokens = tokenizer(text)\n",
        "    print(f\"Tokens para o tokenizer {i+1}: {tokens[:10]} ...\")\n",
        "    with open(f'Shakespeare_Normalized_Tokenized{i+1:02d}.txt', 'w') as outfile:\n",
        "        outfile.write(' '.join(tokens) + '\\n')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hI86nnzDxxoZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25c630cd-8f85-4966-cc04-95dd728e731d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens para o tokenizer 1: ['that', 'poor', 'contempt', 'or', 'claimd', 'thou', 'slept', 'so', 'faithful', 'i'] ...\n",
            "Tokens para o tokenizer 2: ['that', 'poor', 'contempt', 'or', 'claimd', 'thou', 'slept', 'so', 'faithful', 'i'] ...\n",
            "Tokens para o tokenizer 3: ['that', 'poor', 'contempt', 'or', 'claimd', 'thou', 'slept', 'so', 'faithful', 'i'] ...\n",
            "Tokens para o tokenizer 4: ['that', 'poor', 'contempt', 'or', 'claimd', 'thou', 'slept', 'so', 'faithful', 'i'] ...\n",
            "Tokens para o tokenizer 5: ['that', 'poor', 'contempt', 'or', 'claimd', 'thou', 'slept', 'so', 'faithful', 'i'] ...\n",
            "Tokens para o tokenizer 6: ['that', 'poor', 'contempt', 'or', 'claimd', 'thou', 'slept', 'so', 'faithful', 'i'] ...\n",
            "Tokens para o tokenizer 7: ['that', 'poor', 'contempt', 'or', 'claimd', 'thou', 'slept', 'so', 'faithful', 'i'] ...\n",
            "Tokens para o tokenizer 8: ['that', 'poor', 'contempt', 'or', 'claimd', 'thou', 'slept', 'so', 'faithful', '\\n'] ...\n",
            "Tokens para o tokenizer 9: ['that', 'poor', 'contempt', 'or', 'claimd', 'thou', 'slept', 'so', 'faithful', 'i'] ...\n",
            "Tokens para o tokenizer 10: ['that', 'poor', 'contempt', 'or', 'claimd', 'thou', 'slept', 'so', 'faithful', 'i'] ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stop-words Removal"
      ],
      "metadata": {
        "id": "NVGTZb21zKiH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stop(linha):\n",
        "    sw = stopwords.words('english')\n",
        "    tokenized_words = nltk.word_tokenize(linha)\n",
        "    nostopwords_text = [word for word in tokenized_words if not word.lower() in sw]\n",
        "    return ' '.join(nostopwords_text)"
      ],
      "metadata": {
        "id": "N49bTLdKzLZt"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(f'Shakespeare_Normalized_Tokenized02.txt', 'r') as infile:\n",
        "    text = infile.read()\n",
        "cleaned_text = stop(text)\n",
        "with open('Shakespeare_Normalized_Tokenized_StopWord.txt', 'w') as outfile:\n",
        "    outfile.write(cleaned_text)"
      ],
      "metadata": {
        "id": "xmKoRfKl4gTI"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Text Lemmatization"
      ],
      "metadata": {
        "id": "Sutm5DXQ7E8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatization(tokenized_line):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_text = [lemmatizer.lemmatize(word) for word in tokenized_line]\n",
        "    return ' '.join(lemmatized_text)\n"
      ],
      "metadata": {
        "id": "Y3HU2qJo9k8b"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('Shakespeare_Normalized_Tokenized_StopWord.txt', 'r') as infile:\n",
        "    text = infile.read().split()\n",
        "lemmatized_text = lemmatization(text)\n",
        "with open('Shakespeare_Normalized_Tokenized_StopWord_Lemmatized.txt', 'w') as outfile:\n",
        "    outfile.write(lemmatized_text)"
      ],
      "metadata": {
        "id": "OGaRJGV1SE4h"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text Stemming"
      ],
      "metadata": {
        "id": "KB3IHpqvAEev"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Porter Stemmer"
      ],
      "metadata": {
        "id": "kAcoE_pMAQar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stemmerPor(tokenized_line):\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_text = [stemmer.stem(word) for word in tokenized_line]\n",
        "    return ' '.join(stemmed_text)"
      ],
      "metadata": {
        "id": "XDdq-x-lAFGs"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Snowball Stemmer"
      ],
      "metadata": {
        "id": "6ekRibUdAShf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stemmerSnow(tokenized_line):\n",
        "    stemmer = SnowballStemmer(\"english\")\n",
        "    stemmed_text = [stemmer.stem(word) for word in tokenized_line]\n",
        "    return ' '.join(stemmed_text)"
      ],
      "metadata": {
        "id": "tMvjFLvWcm4R"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('Shakespeare_Normalized_Tokenized_StopWord_Lemmatized.txt', 'r') as infile:\n",
        "    text = infile.read().split()\n",
        "stemmed_text_por = stemmerPor(text)\n",
        "with open('Shakespeare_Normalized_Tokenized_StopWord_Lemmatized_Stemming01.txt', 'w') as outfile:\n",
        "    outfile.write(stemmed_text_por)\n",
        "stemmed_text_snow = stemmerSnow(text)\n",
        "with open('Shakespeare_Normalized_Tokenized_StopWord_Lemmatized_Stemming02.txt', 'w') as outfile:\n",
        "    outfile.write(stemmed_text_snow)"
      ],
      "metadata": {
        "id": "6hZSgWtac0Jd"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Análise do Vocabulário"
      ],
      "metadata": {
        "id": "F8eeddMRB2M2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('Shakespeare_Normalized_Tokenized_StopWord_Lemmatized.txt', 'r') as infile:\n",
        "    lemmatized_text = infile.read().split()\n",
        "    lemmatized_counter = Counter(lemmatized_text)\n"
      ],
      "metadata": {
        "id": "p9mDM8bqB4j6"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_tokens_lem = len(lemmatized_counter)\n",
        "avg_occurrences_lem = sum(lemmatized_counter.values()) / total_tokens_lem\n",
        "avg_length_lem = sum(len(token) for token in lemmatized_counter.keys()) / total_tokens_lem\n"
      ],
      "metadata": {
        "id": "YaymIjRM68tN"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('Shakespeare_Normalized_Tokenized_StopWord_Lemmatized_Stemming01.txt', 'r') as infile:\n",
        "    porter_text = infile.read().split()\n",
        "    porter_counter = Counter(porter_text)\n",
        "\n",
        "total_tokens_porter = len(porter_counter)\n",
        "avg_occurrences_porter = sum(porter_counter.values()) / total_tokens_porter\n",
        "avg_length_porter = sum(len(token) for token in porter_counter.keys()) / total_tokens_porter\n"
      ],
      "metadata": {
        "id": "ShoqoRvhdVzL"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('Shakespeare_Normalized_Tokenized_StopWord_Lemmatized_Stemming02.txt', 'r') as infile:\n",
        "    snowball_text = infile.read().split()\n",
        "    snowball_counter = Counter(snowball_text)\n",
        "\n",
        "total_tokens_snow = len(snowball_counter)\n",
        "avg_occurrences_snow = sum(snowball_counter.values()) / total_tokens_snow\n",
        "avg_length_snow = sum(len(token) for token in snowball_counter.keys()) / total_tokens_snow\n"
      ],
      "metadata": {
        "id": "gtMTz39RdZtC"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('Shakespeare_Vocabulary_Analysis.txt', 'w') as analysis_file:\n",
        "    # lemmmm\n",
        "    analysis_file.write(f\"Lemmatizer:\\n\")\n",
        "    analysis_file.write(f\"Tamanho do vocabulário: {total_tokens_lem}\\n\")\n",
        "    analysis_file.write(f\"Número médio de ocorrências: {avg_occurrences_lem}\\n\")\n",
        "    analysis_file.write(f\"Tamanho médio dos tokens: {avg_length_lem}\\n\\n\")\n",
        "\n",
        "\n",
        "    # proter\n",
        "    analysis_file.write(f\"Porter Stemmer:\\n\")\n",
        "    analysis_file.write(f\"Tamanho do vocabulário: {total_tokens_porter}\\n\")\n",
        "    analysis_file.write(f\"Número médio de ocorrências: {avg_occurrences_porter}\\n\")\n",
        "    analysis_file.write(f\"Tamanho médio dos tokens: {avg_length_porter}\\n\\n\")\n",
        "\n",
        "    # Snowball\n",
        "    analysis_file.write(f\"Snowball Stemmer:\\n\")\n",
        "    analysis_file.write(f\"Tamanho do vocabulário: {total_tokens_snow}\\n\")\n",
        "    analysis_file.write(f\"Número médio de ocorrências: {avg_occurrences_snow}\\n\")\n",
        "    analysis_file.write(f\"Tamanho médio dos tokens: {avg_length_snow}\\n\\n\")"
      ],
      "metadata": {
        "id": "H0bLhXtcdn_P"
      },
      "execution_count": 68,
      "outputs": []
    }
  ]
}