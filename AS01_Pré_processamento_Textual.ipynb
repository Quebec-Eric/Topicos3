{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Aluno:Eric Azevedo de Oliveira\n"
      ],
      "metadata": {
        "id": "4idr7NaVRoXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#imports para normalizacao\n",
        "import unicodedata\n",
        "import re\n",
        "\n",
        "#imports para tokenizacao\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import spacy\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.tokenize import MWETokenizer\n",
        "from textblob import TextBlob\n",
        "from gensim.utils import tokenize\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "\n",
        "# Stio words Removal\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n",
        "# stemmer\n",
        "\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "# 6 parte\n",
        "import pandas as pd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yX6UGn4WXDO4",
        "outputId": "61276ff3-0c96-42a5-920d-7e1dea21bcb8"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lower case"
      ],
      "metadata": {
        "id": "8LIbwTMRQBYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fazerLowerCasa(linhaArquivo):\n",
        "    return linhaArquivo.lower()"
      ],
      "metadata": {
        "id": "N7xICNOKQDMV"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accent and diacritic removal"
      ],
      "metadata": {
        "id": "7EDGKHtUQPgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accentAndDiacriticRemoval(linhaArquivo):\n",
        "    nfkd_from=unicodedata.normalize('NFKD',linhaArquivo)\n",
        "    return u\"\".join([c for c in nfkd_from if not unicodedata.combining(c)])\n"
      ],
      "metadata": {
        "id": "KRqXVaCWVJ8F"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Canonicalizing of acronyms, currency, date and hyphenated words"
      ],
      "metadata": {
        "id": "92jBv023YZzr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def canonicalizing(linhaArquivo):\n",
        "    linhaArquivo = re.sub('\\.(?!(\\S[^. ])|\\d)','', linhaArquivo)\n",
        "    linhaArquivo = re.sub('(?<!\\d)[.,;!?\\'\\(\\)#:-](?!\\d)','',linhaArquivo)\n",
        "    return linhaArquivo\n",
        "\n"
      ],
      "metadata": {
        "id": "PwMD89shYaoG"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalização"
      ],
      "metadata": {
        "id": "fJULOW-HeuKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Leitura do arquivo\n",
        "\n",
        "with open('Shakespeare.txt', 'r') as arquivoLeitura, open(\"Shakespeare_Normalized.txt\", \"w\") as arquivoEscrita:\n",
        "  for linha in arquivoLeitura:\n",
        "      linha=fazerLowerCasa(linha)\n",
        "      linha=accentAndDiacriticRemoval(linha)\n",
        "      linha=canonicalizing(linha)\n",
        "      arquivoEscrita.write(linha)\n",
        "      arquivoEscrita.flush()\n",
        "\n",
        "arquivoLeitura.close()\n",
        "arquivoEscrita.close()\n"
      ],
      "metadata": {
        "id": "1IlNXQInevU0"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Tokenização inicio"
      ],
      "metadata": {
        "id": "w2BwASabj8bO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "White Space Tokenization"
      ],
      "metadata": {
        "id": "SFCLxC1bj-od"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def whiteSpace(linhaArquivo):\n",
        "    return linhaArquivo.split()"
      ],
      "metadata": {
        "id": "1jWJ52PHkAjX"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK: Word Tokenizer"
      ],
      "metadata": {
        "id": "gl9hfbbHlqEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def wordTokenizer(linhaArquivo):\n",
        "    return word_tokenize(linhaArquivo)"
      ],
      "metadata": {
        "id": "SbwC7mYhlq2d"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK: Tree Bank Tokenizer"
      ],
      "metadata": {
        "id": "wPQdkZDolsAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def treeBankokenizer(linhaArquivo):\n",
        "    token=TreebankWordTokenizer()\n",
        "    return token.tokenize(linhaArquivo)"
      ],
      "metadata": {
        "id": "fwLkAJxlltqs"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK: Word Punctuation Tokenizer"
      ],
      "metadata": {
        "id": "6jD2Lz4Blt3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def wordPunctuationTokenizer(linhaArquivo):\n",
        "    toke=WordPunctTokenizer()\n",
        "    return toke.tokenize(linhaArquivo)"
      ],
      "metadata": {
        "id": "Dyg9NeZlluiS"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK: Tweet Tokenizer"
      ],
      "metadata": {
        "id": "-KgmbgfXlvZh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tweetTokenizer(linhaArquivo):\n",
        "    toke= TweetTokenizer()\n",
        "    return toke.tokenize(linhaArquivo)"
      ],
      "metadata": {
        "id": "CxnWqFOelzwZ"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK: MWE Tokenizer"
      ],
      "metadata": {
        "id": "V00m2mfCl1U4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mweTokenizer(linhaArquivo):\n",
        "  return linhaArquivo"
      ],
      "metadata": {
        "id": "STW3YXbCl1qo"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TextBlob Word Tokenizer"
      ],
      "metadata": {
        "id": "0szjnlN3l3ss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def textBlobWordTokenize(linhaArquivo):\n",
        "    text = TextBlob(linhaArquivo)\n",
        "    return text.words\n"
      ],
      "metadata": {
        "id": "dEJbY1Xkl4Mf"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "spaCy Tokenizer"
      ],
      "metadata": {
        "id": "Fv2FMLtyl5Em"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def spacyTokenizer(linhaArquivo):\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    dc=nlp(linhaArquivo)\n",
        "    resposta=[token.text for token in dc]\n",
        "    return resposta"
      ],
      "metadata": {
        "id": "0uC2Q1t9l5kO"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gensim Word Tokenizer"
      ],
      "metadata": {
        "id": "EZSM-SO5l64Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gensimWordTokenizer(linhaArquivo):\n",
        "    return list(tokenize(linhaArquivo))"
      ],
      "metadata": {
        "id": "YYBFc0cPl7QR"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keras Tokenization"
      ],
      "metadata": {
        "id": "OqzExtzQl7vg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def kerasTokenization(linhaArquivo):\n",
        "    return text_to_word_sequence(linhaArquivo)"
      ],
      "metadata": {
        "id": "-P7f1amNl9fs"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#enum das funcoes\n",
        "tokes= [\n",
        "    whiteSpace,\n",
        "    wordTokenizer,\n",
        "    treeBankokenizer,\n",
        "    wordPunctuationTokenizer,\n",
        "    tweetTokenizer,\n",
        "    mweTokenizer,\n",
        "    textBlobWordTokenize,\n",
        "    spacyTokenizer,\n",
        "    gensimWordTokenizer,\n",
        "    kerasTokenization\n",
        "]"
      ],
      "metadata": {
        "id": "CHHPmAeRvsYE"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('Shakespeare_Normalized.txt', 'r') as infile:\n",
        "    text = infile.read()\n",
        "\n",
        "for i, tokenizer in enumerate(tokes):\n",
        "    tokens = tokenizer(text)\n",
        "    with open(f'Shakespeare_Normalized_Tokenized{i+1:02d}.txt', 'w') as outfile:\n",
        "        outfile.write(' '.join(tokens))"
      ],
      "metadata": {
        "id": "hI86nnzDxxoZ"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stop-words Removal"
      ],
      "metadata": {
        "id": "NVGTZb21zKiH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stop(linha):\n",
        "    sw = stopwords.words('english')\n",
        "    tokenized_words = word_tokenize(linha)\n",
        "    nostopwords_text = [word for word in tokenized_words if not word.lower() in sw]\n",
        "    return ' '.join(nostopwords_text)\n"
      ],
      "metadata": {
        "id": "N49bTLdKzLZt"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('Shakespeare_Normalized_Tokenized02.txt', 'r') as arquivoLeitura, open(\"Shakespeare_Normalized_Tokenized_StopWord.txt\", \"w\") as arquivoEscrita:\n",
        "    for linha in arquivoLeitura:\n",
        "        linha = stop(linha)\n",
        "        arquivoEscrita.write(linha + '\\n')"
      ],
      "metadata": {
        "id": "xmKoRfKl4gTI"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Text Lemmatization"
      ],
      "metadata": {
        "id": "Sutm5DXQ7E8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatization(linha):\n",
        "  lemmatizer=WordNetLemmatizer()\n",
        "  lemmatized_text=[lemmatizer.lemmatize(word) for word in linha]\n",
        "  return lemmatized_text"
      ],
      "metadata": {
        "id": "Y3HU2qJo9k8b"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text Stemming"
      ],
      "metadata": {
        "id": "KB3IHpqvAEev"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Porter Stemmer"
      ],
      "metadata": {
        "id": "kAcoE_pMAQar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stemmerPor(linha):\n",
        "    stemmer= PorterStemmer()\n",
        "    stemmer_text=[stemmer.stem(word)for word in linha]\n",
        "    return stemmer_text"
      ],
      "metadata": {
        "id": "XDdq-x-lAFGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Snowball Stemmer"
      ],
      "metadata": {
        "id": "6ekRibUdAShf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "def stemmerSnow(linha):\n",
        "    stemmer=SnowballStemmer(\"english\")\n",
        "    stemmed_text= [stemmer.stem(word) for  word in linha]\n",
        "    return linha"
      ],
      "metadata": {
        "id": "-G18eEmjATGc"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Análise do Vocabulário"
      ],
      "metadata": {
        "id": "F8eeddMRB2M2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_csv(counter, filename):\n",
        "    df = pd.DataFrame(counter.items(), columns=['Token', 'Frequency'])\n",
        "    df['Length'] = df['Token'].apply(len)\n",
        "    df.to_csv(filename, index=False)\n"
      ],
      "metadata": {
        "id": "p9mDM8bqB4j6"
      },
      "execution_count": 86,
      "outputs": []
    }
  ]
}